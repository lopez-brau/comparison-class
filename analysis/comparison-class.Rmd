---
title: "Inferring comparison classes through pragmatic reasoning"
author: "Michael Lopez-Brau"
knit: (function(inputFile, encoding) { 
  out_dir <- "..";
  rmarkdown::render(inputFile,
                  encoding=encoding, 
                  output_file=file.path(dirname(inputFile), out_dir, "index.html")) })
output:
  html_document:
    fig_caption: yes
    highlight: pygments
    toc: yes
    collapsed: false
    toc_float: false
fontsize: 11pt
geometry: margin=1in
header-includes:
- \usepackage{subfig}
- \usepackage{float}
- \usepackage{amssymb}
fontfamily: times
bibliography: comparison-class.bib
nocite: |
  @bale2011scales,
  @ebeling1994children,
  @lassiter2013context
---

```{r setup, include = FALSE}
# set the local path of the comparison-class folder
path = "~/cocolab/comparison-class/"
#path = "~/Documents/research/comparison-class/"

# import required libraries
library(knitr)
library(ggplot2)
library(cowplot)
library(pander)
```

# Introduction

Suppose you walk into a coffee shop and overhear someone say “the coffee is warm.” What do you think they meant by that? If the person is drinking a [cappuccino](https://en.wikipedia.org/wiki/Cappuccino), warm might mean around 60&deg;C or 160&deg;F. In contrast, if the person is drinking an [iced coffee](https://en.wikipedia.org/wiki/Iced_coffee), warm might mean around 15&deg;C or 59&deg;F. It is apparent that in order for us to ground a quantitative interpretation of the gradable adjective “warm,” we need to know the set of entities that coffee belongs to from which we can derive the standard of "warm." In other words, we are interested in finding the set that contains other entities that make the coffee in this particular utterance "warm": we are interested in finding the comparison class.

## What are comparison classes?

**Comparison classes** are the implicitly or textually determined sets that relativize the standard for some gradable adjective. Formally, in our previous example, the comparison class would have been either the $C$, set of all cappuccinos or $I$, the set of all iced coffees. From the previous vignette and using the approach from @bale2011scales we can set up a _quasi-order_ of "warm," a relation on the comparison class, as follows:

$$\begin{equation}
\text{WARM} = \{(x, y) : x \text{ is (at least) as warm as } y\}
\end{equation}$$

Though this may seem trivial when taking the given context into account, we will show some non-trivial examples in the results section.

## What role do comparison classes play in understanding gradable adjectives?

The comparison class also restricts the set of possible meanings of a gradable adjective. For example, it would probably not make sense to describe a warm cappuccino as being 60&deg;F. Alternatively, the value "60&deg;F" would not appear in the set that contains the possible meanings of "warm."

The question of what warm means and how to infer its meaning, as well as many other _gradable adjectives_, is an active area of research. There are two types of gradable adjectives: relative and absolute. In our work, we focus on relative gradable adjectives (e.g., tall, heavy, hot) because of their vagueness and context-sensitivity. This allows us to study how the comparison class varies with respect to context while keeping the utterance fixed; thus, we obtain more flexibility in trying to elicit the comparison class in our experiments.

While previous work has focused on building computational models to infer the meaning of such gradable adjectives, the comparison class has always been assumed _a priori_. A good computational model of human inference of gradable adjectives is a model that can first infer the comparison class in order to make an accurate quantitative interpretation of the gradable adjective. 

# Experiment

The pilot experiment was run on Amazon Mechanical Turk (MTurk) with 30 participants. Each participant had to answer 18 questions in a fill-in-the-blank and paraphrase format. The experiment took about five minutes and participants were paid $0.60. You can find the experiment [here](https://mhtess.github.io/comparison-class/).

Here is what a typical experiment trial would look like: 

> Bill is a goat. John says, "Bill is tall." What do you think John meant? "Bill is tall for a `goat`."

The first and second sentences are the context and target sentences, respectively. The 18 questions were unique pairs of these sentences. The third and fourth sentence are the prompts, where the participant gives an answer. The name(s) were assigned to each trial randomly and uniquely from a pool of names normalized for race [citation needed].

In addition, each participant was randomly assigned a condition that changed the way the final line was phrased. The two conditions were "for a" and "relative to." Given the fact that responses like "Drink" and "drink" might be miscategorized in separate bins, we compare responses that were parsed by hand with the [caveman](https://github.com/erindb/caveman) parser. In the next section, we graph the results of peoples' responses and finish with a brief analysis and discussion in the section thereafter.

# Results

In the graphs below, the names of some of the bins were cut down due to space limitations. The full respones can be read from [here](https://github.com/lopez-brau/comparison-class/blob/master/data/pilot-1-paraphrase/pilot-1-paraphrase-pairs.csv).

## "for a" Condition

```{r fig.align = "center", fig.height = 6, fig.width = 8, echo = FALSE, results = "asis"}
source(paste(path, "scripts/make-plots-condition-1.R", sep = ""))
```

## "relative to" Condition

```{r fig.align = "center", fig.height = 6, fig.width = 8, echo = FALSE, results = "asis"}
source(paste(path, "scripts/make-plots-condition-2.R", sep = ""))
```

# Analysis

## "for a" Condition

* Pair 6: The top two answers for this pair were "diamond necklace" and "diamond." While the former makes sense, given the context, the latter does not. This may have been a mistake rather than an honest interpretation but more data would be needed to confirm if this pair elicits the comparison class "diamond" for the subject "diamond necklace."
* Pair 7: Most participants chose "movie" instead of "movie ticket," resulting in the following sentence: "The ticket is cheap for a movie." It seems that the subject "ticket" elicits the comparison class "movies," or more generally the price to see a movie.
* Pair 9: I chose to separate "road trip" and "drive" because the former has the connotation of long trips where the latter is less inclined to any particular length.
* Pair 11: "doctor's office" and "doctor's appointment" were the two most common answers but elicit different comparison classes. With regard to the "doctor's appointment" response, it seems that participants here relied too heavily on the context.
* Pair 13 & 14: The top answer was "4-year-old" for the first pair and "man/woman" for the second. This is likely because the distributions of heights among 3-, 4-, and 5-year-olds are much more different than the distributions of heights of 24-, 25-, and 26-year-old adults; hence, it would be more informative to communicate the direct age of toddler than an adult when talking about height.

## "relative to" Condition

In general, most of the pairs in this condition experienced too much variability in responses. More of this will be touched on in the next section.

### Word usage

Participants in the "relative to" condition habitually made use of the words "other" and "typical." A chart with this data is shown below:

```{r fig.align = "center", echo = FALSE, results = "asis"}
source(paste(path, "scripts/count-words.R", sep = ""))
panderOptions('table.split.table', Inf)
cat("<center>", sep = "")
pandoc.table(chart)
cat("</center>", sep = "")
```

<center>Table 1: Word frequency data from the "relative to" condition</center><br>

The columns of the table correspond to:

* other.count: the number of times "other" occurred
* typical.count: the number of times the word "typical" occurred
* other.frequency: the frequency that the word "other" occurred (other.count / responses.total)
* typical.frequency: the frequency that the word "typical" occurred (typical.count / responses.total)



# Discussion

Looking at the data above, a stark difference between the conditions is the variability in responses: the “relative to” condition elicted far more unique responses than the "for a" condition. Also, the “relative to” condition elicited noun phrases, such as "other 4-year-olds," rather than nouns, such as “4-year-olds,” as in the "for a" condition.

To tackle variability in future experiments, we plan to look at how participant responses are encoded. Should the phrases "doctor's appointment" and "doctor's office" be encoded as similar or different responses? Hand-encoding responses can lead to large variations in results, leading to data that is not reproducible. We seek to apply a formal tool for encoding responses in future experiments.

To best elicit the comparison class in our experiments, we have decided to update our "relative to" condition with a "relative to a" condition. Once we improve our experiment design and obtain these elicited classes, we can then measure participants' background knowledge and interpretations using these contextual manipulations, incorporate it with the adjectives model in [1], and see if the knowledge from the classes we measured makes the right predictions for interpreting gradable adjectives.

# References
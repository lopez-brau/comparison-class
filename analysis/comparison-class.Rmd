---
title: "Inferring comparison classes through pragmatic reasoning"
author: "Michael Lopez-Brau"
knit: (function(inputFile, encoding) { 
  out_dir <- "..";
  rmarkdown::render(inputFile,
                  encoding=encoding, 
                  output_file=file.path(dirname(inputFile), out_dir, "index.html")) })
output:
  html_document:
    title: lol
    fig_caption: yes
    highlight: pygments
    toc: yes
    collapsed: false
    toc_float: false
fontsize: 11pt
geometry: margin=1in
header-includes:
- \usepackage{subfig}
- \usepackage{float}
- \usepackage{amssymb}
fontfamily: times
#bibliography: bibliography.bib
---

```{r setup, echo = FALSE}
# set the local path of the comparison-class folder
path = "~/cocolab/comparison-class/"

# import required libraries
library(ggplot2)
library(knitr)
```

<!-- # Introduction

Ontological knowledge refers to one's conception of the basic categories of existence: of what sorts of things there are. These basic categories, or classes, allow us to establish relations between objects, events, or people. In Frank C. Keil's book, Semantic and Conceptual Development, he lays out a framework for a possible theory of the structure of ontological knowledge, simply called Sommers' theory. Specifically, the theory entails two hierarchical trees, where one describes the relationship between predicates and the terms they can be used with while the other describes the ontological classes that encapsulate similar predicates. With this structure in mind, the more general classes and predicates rest towards the top (e.g. living things, events) and specific classes much lower (e.g. dogs, motorcycles). Even though his theory is centered around the relationship between predicates and terms, we can easily draw parallels to features and kinds.

Without getting into excessive detail, Sommer's theory claims that there are four psychological phenomena that are surface manifestations of underlying ontological knowledge. For this reason, Keil makes a point to ensure that the model satisfies these phenomena. The descriptions of how the phenomena are satisfied have been modified to reflect the change from predicates and terms to features and kinds.

* Anomalous sentences - sensible and nonsensible features are dealt with by tree structure. If a feature-kind is sensible, then the feature will dominate that kind in the tree. Conversely, if the feature-kind is nonsensible, the feature will not dominate the kind. "Dominate," in this sense,
means to be above it in the tree.
* Natural classes - for a class to be natural, it must exhaustively dominate all and only the kinds in that class. Conversely, non-natural classes are those classes that include kinds on different branches but fail to include all kinds that are common to those branches. The notion of naturalness used here defines ontological classes that can be collapsed into larger classes that also form coherent ontological structures.
* Similarity - classes of kinds that are closely dominated by a common feature are more similar to each other than classes that are distantly dominated. The relation is also transitive: the theory can make similarity predictions between pairs a-b and a-c but is unable to predict relative similarities between pairs a-b and c-d.
* Copredication - a natural copredication is when two predicates (features, in our case) are connected through the classical two-place logical connectives, "and," "or," and "if...then," and are sensible. The notion of natural here is synonymous with sensible. Nodes that are on a "common line," or share a branch, are copredicable. Here is an example of a natural copredication: X is tall and X is red. For clarity, here is an example of a non-natural copredication: Either X is fat or X is false.

Generic sentences, also known as generics, are generalizations about the prevalence of features. Here is an example of a generic: Dogs have fur. Clearly, there is some ambiguity here--how many dogs have fur? Though the threshold of prevalence is unknown, it is approximated by the listener through inference. Another interesting question can be derived from this generic: what could I have been comparing dogs to that I had to specify that they had fur (e.g. other animals)? In other words, what is the contrast class of "dogs," given this feature? A contrast class can be seen as a set of possible candidates for comparison with the kind. For this project, the latter question is of interest to us. We would like to use WebPPL and several elements of Sommers' theory to build a computational model that can infer the contrast class, conditioned on a feature, of a kind.

It is helpful to think of this problem in terms of inference due to the nature of natural language. Because natural language can be noisy, there can be uncertainty about the information a listener receives. Consequently, deterministic approaches to the problem will not suffice. On the other hand, a Bayesian model best capitalizes on the fact that, as humans, we have prior beliefs about the world that aid us in our everyday inference tasks. Specifically, our knowledge about the propensity of certain classes over others can help us factor in a prior that is used in our day-to-day conversations. -->

# Introduction

Suppose you walk into a coffee shop and overhear someone say “the coffee is warm.” What do you think they meant by that? If the person is drinking a [cappuccino](https://en.wikipedia.org/wiki/Cappuccino), warm might mean which is around 60&deg;C or 160&deg;F. In contrast, if the person is drinking an [iced coffee](https://en.wikipedia.org/wiki/Iced_coffee), warm might mean around 15&deg;C or 59&deg;F.

It is apparent that the meaning of the entire utterance, not just the gradable adjective, is quite flexible. In order for us to ground a quantitative interpretation of the adjective “warm,” we need to know what we are comparing coffee to: we need to know the comparison class.

## What are comparison classes?

Given an utterance and usually some context, a **comparison class** is the set of candidate entities that are used for comparison with the subject. Formally, in our previous example, the comparison class would have been either the $\mathbb{C}$, set of all cappuccinos or $\mathbb{I}$, the set of all iced coffees. Though this may seem trivial when taking the given context into account, we will show some non-trivial examples in the results section.

## What role do comparison classes play in understanding gradable adjectives?

The comparison class also restricts the set of possible meanings of a gradable adjective. For example, it would probably not make sense to describe a warm cappuccino as being 60&deg;F. Alternatively, the value "60&deg;F" would not appear in the set that contains the possible meanings of "warm."

The question of what warm means and how to infer its meaning, as well as many other _gradable adjectives_, is an active area of research. There are two types of gradable adjectives: relative and absolute. In our work, we focus on relative gradable adjectives (e.g., tall, heavy, hot) because of their vagueness and context-sensitivity. This allows us to study how the comparison class varies with respect to context while keeping the utterance fixed; thus, we obtain more flexibility in trying to elicit the comparison class in our experiments.

While previous work has focused on building computational models to infer the meaning of such gradable adjectives, the comparison class has always been assumed _a priori_. A good computational model of human inference of gradable adjectives is a model that can first infer the comparison class in order to make an accurate quantitative interpretation of the gradable adjective. 

# Experiment

The pilot experiment was run on Amazon Mechanical Turk (MTurk) with 30 participants. Each participant had to answer 18 questions in a fill-in-the-blank and paraphrase format. The experiment took about five minutes and participants were paid $0.60. You can find the experiment [here](https://mhtess.github.io/comparison-class/).

Here is what a typical experiment trial would look like: 

> Bill is a goat. John says, "Bill is tall." What do you think John meant? "Bill is tall for a `goat`."

The first and second sentences are the context and target sentences, respectively. The 18 questions were unique pairs of these sentences. The third and fourth sentence are the prompts, where the participant gives an answer. The name(s) were assigned to each trial randomly and uniquely from a pool of names normalized for race [citation needed].

In addition, each participant was randomly assigned a condition that changed the way the final line was phrased. The two conditions were "for a" and "relative to." Given the fact that responses like "Drink" and "drink" might be miscategorized in separate bins, we compare responses that were parsed by hand with the [Caveman](https://github.com/erindb/caveman) parser. In the next section, we graph the results of peoples' responses and finish with a brief analysis and discussion in the section thereafter.

# Results

In the graphs below, the names of some of the bins were cut down due to space limitations. The full respones can be read from [here](https://github.com/lopez-brau/comparison-class/blob/master/data/pilot-1-paraphrase/pilot-1-paraphrase-pairs.csv).

## "for a" Condition {.tabset}

### Manual Parsing

```{r, echo = FALSE, fig.height = 3.4, fig.width = 3.4, results = "asis"}
# set the working directory to the raw data
dataPath = paste(path, "data/pilot-1-paraphrase/processed/pilot-1-paraphrase-trials-condition-1/", sep = "")

# get the number of files in the current condition folder
files = list.files(dataPath)

for (pair in 1:length(files)) {
  # read in the csv file
  file = read.csv(paste(dataPath, "pair-", as.character(pair) ,"-condition-1", sep = ""))
  file["response"] = data.frame(substr(file[,"response"], 1, 20))
  colnames(file["response"]) = c("response")
  
  # generates the base plot
  pairs = read.csv(paste(dataPath, "../../pilot-1-paraphrase-pairs.csv", sep = ""))
  build = ggplot_build(ggplot(data = file["response"], aes(x = file[,"response"])) + geom_bar())
  response = build$panel$ranges[[1]]$x.labels
  response = data.frame(response)
  count = build$data[[1]]$count
  count = data.frame(count)

  # prints the plot
  Target = paste(as.character(pairs[,"target"][pair]), sep = "")
  Context = paste(as.character(pairs[,"context"][pair]), sep = "")
  print(kable(x = data.frame(Target, Context), format = "pandoc"))
  writeLines("\n")
  title = paste("Pair ", as.character(pair), sep = "")
  plot = ggplot(data = data.frame(response, count), aes(x = reorder(response, count), y = count)) + ylab("count") + xlab("response") + ggtitle(title) + geom_bar(stat = "identity") + coord_flip()
  print(plot)

  # make a table of the data
  table = data.frame(response = levels(reorder(response, sort.list(count[,"count"], decreasing = TRUE))), count = count[sort.list(count[,"count"], decreasing = TRUE),])
  #print(kable(x = table, format = "markdown"))
}
```

### Caveman Parsing

```{r, echo = FALSE, fig.height = 3.4, fig.width = 3.4, results = "asis"}
# set the working directory to the raw data
dataPath = paste(path, "data/pilot-1-paraphrase/caveman/pilot-1-paraphrase-trials-condition-1/", sep = "")

# get the number of files in the current condition folder
files = list.files(dataPath)

for (pair in 1:length(files)) {
  # read in the csv file
  file = read.csv(paste(dataPath, "pair-", as.character(pair) ,"-condition-1", sep = ""), header = FALSE)
  response = file[,"V1"]
  file = data.frame(response)
  file["response"] = data.frame(substr(file[,"response"], 1, 20))
  colnames(file["response"]) = c("response")
  
  # generates the base plot
  pairs = read.csv(paste(dataPath, "../../pilot-1-paraphrase-pairs.csv", sep = ""))
  build = ggplot_build(ggplot(data = file["response"], aes(x = file[,"response"])) + geom_bar())
  response = build$panel$ranges[[1]]$x.labels
  response = data.frame(response)
  count = build$data[[1]]$count
  count = data.frame(count)

  # prints the plot
  Target = paste(as.character(pairs[,"target"][pair]), sep = "")
  Context = paste(as.character(pairs[,"context"][pair]), sep = "")
  print(kable(x = data.frame(Target, Context), format = "pandoc"))
  writeLines("\n")
  title = paste("Pair ", as.character(pair), sep = "")
  plot = ggplot(data = data.frame(response, count), aes(x = reorder(response, count), y = count)) + ylab("count") + xlab("response") + ggtitle(title) + geom_bar(stat = "identity") + coord_flip()
  print(plot)

  # make a table of the data
  table = data.frame(response = levels(reorder(response, sort.list(count[,"count"], decreasing = TRUE))), count = count[sort.list(count[,"count"], decreasing = TRUE),])
  #print(kable(x = table, format = "markdown"))
}
```

## "relative to" Condition {.tabset}

### Manual Parsing

```{r, echo = FALSE, fig.height = 3.4, fig.width = 3.4, results = "asis"}
# set the working directory to the raw data
dataPath = paste(path, "data/pilot-1-paraphrase/processed/pilot-1-paraphrase-trials-condition-2/", sep = "")

# get the number of files in the current condition folder
files = list.files(dataPath)

for (pair in 1:length(files)) {
  # read in the csv file
  file = read.csv(paste(dataPath, "pair-", as.character(pair) ,"-condition-2", sep = ""))
  file["response"] = data.frame(substr(file[,"response"], 1, 20))
  colnames(file["response"]) = c("response")
  
  # generates the base plot
  pairs = read.csv(paste(dataPath, "../../pilot-1-paraphrase-pairs.csv", sep = ""))
  build = ggplot_build(ggplot(data = file["response"], aes(x = file[,"response"])) + geom_bar())
  response = build$panel$ranges[[1]]$x.labels
  response = data.frame(response)
  count = build$data[[1]]$count
  count = data.frame(count)
  
  # prints the plot
  Target = paste(as.character(pairs[,"target"][pair]), sep = "")
  Context = paste(as.character(pairs[,"context"][pair]), sep = "")
  print(kable(x = data.frame(Target, Context), format = "pandoc"))
  writeLines("\n")
  title = paste("Pair ", as.character(pair), sep = "")
  plot = ggplot(data = data.frame(response, count), aes(x = reorder(response, count), y = count)) + ylab("count") + xlab("response") + ggtitle(title) + geom_bar(stat = "identity") + coord_flip()
  print(plot)
  
  # make a table of the data
  table = data.frame(response = levels(reorder(response, sort.list(count[,"count"], decreasing = TRUE))), count = count[sort.list(count[,"count"], decreasing = TRUE),])
  #print(kable(x = table, format = "markdown"))
}
```

### Caveman Parsing

```{r, echo = FALSE, fig.height = 3.4, fig.width = 3.4, results = "asis"}
# set the working directory to the raw data
dataPath = paste(path, "data/pilot-1-paraphrase/caveman/pilot-1-paraphrase-trials-condition-2/", sep = "")

# get the number of files in the current condition folder
files = list.files(dataPath)

for (pair in 1:length(files)) {
  # read in the csv file
  file = read.csv(paste(dataPath, "pair-", as.character(pair) ,"-condition-2", sep = ""), header = FALSE)
  response = file[,"V1"]
  file = data.frame(response)
  file["response"] = data.frame(substr(file[,"response"], 1, 20))
  colnames(file["response"]) = c("response")
  
  # generates the base plot
  pairs = read.csv(paste(dataPath, "../../pilot-1-paraphrase-pairs.csv", sep = ""))
  build = ggplot_build(ggplot(data = file["response"], aes(x = file[,"response"])) + geom_bar())
  response = build$panel$ranges[[1]]$x.labels
  response = data.frame(response)
  count = build$data[[1]]$count
  count = data.frame(count)

  # prints the plot
  Target = paste(as.character(pairs[,"target"][pair]), sep = "")
  Context = paste(as.character(pairs[,"context"][pair]), sep = "")
  print(kable(x = data.frame(Target, Context), format = "pandoc"))
  writeLines("\n")
  title = paste("Pair ", as.character(pair), sep = "")
  plot = ggplot(data = data.frame(response, count), aes(x = reorder(response, count), y = count)) + ylab("count") + xlab("response") + ggtitle(title) + geom_bar(stat = "identity") + coord_flip()
  print(plot)

  # make a table of the data
  table = data.frame(response = levels(reorder(response, sort.list(count[,"count"], decreasing = TRUE))), count = count[sort.list(count[,"count"], decreasing = TRUE),])
  #print(kable(x = table, format = "markdown"))
}
```

# Analysis

## "for a" Condition

* Pair 6: The top two answers for this pair were "diamond necklace" and "diamond." While the former makes sense, given the context, the latter does not. This may have been a mistake rather than an honest interpretation but more data would be needed to confirm if this pair elicits the comparison class "diamond" for the subject "diamond necklace."
* Pair 7: Most participants chose "movie" instead of "movie ticket," resulting in the following sentence: "The ticket is cheap for a movie." It seems that the subject "ticket" elicits the comparison class "movies," or more generally the price to see a movie.
* Pair 9: I chose to separate "road trip" and "drive" because the former has the connotation of long trips where the latter is less inclined to any particular length.
* Pair 11: "doctor's office" and "doctor's appointment" were the two most common answers but elicit different comparison classes. With regard to the "doctor's appointment" response, it seems that participants here relied too heavily on the context.
* Pair 13 & 14: The top answer was "4-year-old" for the first pair and "man/woman" for the second. This is likely because the distributions of heights among 3-, 4-, and 5-year-olds are much more different than the distributions of heights of 24-, 25-, and 26-year-old adults; hence, it would be more informative to communicate the direct age of toddler than an adult when talking about height.

## "relative to" Condition

In general, most of the pairs in this condition experienced too much variability in responses. More of this will be touched on in the next section.

# Discussion

Looking at the data above, a stark difference between the conditions is the variability in responses: the “relative to” condition elicted far more unique responses than the "for a" condition. Also, the “relative to” condition elicited noun phrases, such as "other 4-year-olds," rather than nouns, such as “4-year-olds,” as in the "for a" condition.

To tackle variability in future experiments, we plan to look at how participant responses are encoded. Should the phrases "doctor's appointment" and "doctor's office" be encoded as similar or different responses? Hand-encoding responses can lead to large variations in results, leading to data that is not reproducible. We seek to apply a formal tool for encoding responses in future experiments.

To best elicit the comparison class in our experiments, we have decided to update our "relative to" condition with a "relative to a" condition. Once we improve our experiment design and obtain these elicited classes, we can then measure participants' background knowledge and interpretations using these contextual manipulations, incorporate it with the adjectives model in [1], and see if the knowledge from the classes we measured makes the right predictions for interpreting gradable adjectives.

# References
[1] D. Lassiter, & N. D. Goodman. 2013. Context, scale structure, and statistics in the interpretation of positive-form adjectives. _In Semantics and Linguistic Theory_ (SALT) 23.